---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

<br><br>
# üëÄ About Me

üå± I‚Äôm **Hui Huang**, a 4th year Ph.D. student at Harbin Institute of Technology in Computer Science.

üìñ My research interest includes:
  - Large Language Models (LLM) 
  - Natural Language Processing (NLP) 

üì´ Feel free to contact me via email: 22b903058@stu.hit.edu.cn.

<br><br>
# üìñ Educations
- *2022.08 - Present*, Ph.D., Harbin Institute of Technology in Computer Science. Supervised by Professor [Muyun Yang](https://homepage.hit.edu.cn/yangmuyun).
- *2018.09 - 2022.06*, M.Sc., Beijing Jiaotong University in Computer Science. Supervised by Professor [Jin'an Xu](https://faculty.bjtu.edu.cn/8300/).
- *2014.09 - 2018.06*, B.Sc., University of Science and Technology Beijing in Computer Science.

<br><br>
# üíª Experiences
- *2025.07 - Present*, Visiting Student, [Arase Lab](https://arase-cl-lab.c.titech.ac.jp/), Institute of Science Tokyo.
- *2024.07 - 2025.06*, Research Internship, Future Life Laboratory, Alibaba Group.
- *2023.09 - 2024.03*, Research Internship, Department of Wenxin Yiyan, Baidu Inc.. 
- *2023.03 - 2023.08*, Research Internship, AI Lab, ByteDance Inc.. 
- *2022.10 - 2023.03*, Research Internship, Lark AI, ByteDance Inc.. 
- *2019.07 - 2022.08*, Research Internship, Toshiba Research and Development Center. 

<br><br>
# üìù Selected Publications

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Musc: Improving complex instruction following with multi-granularity self-contrastive training.**

**H. Huang**, J. Liu, Y. He, S. Li, B. Xu, C. Zhu, M. Yang, T. Zhao. In ***Proc. of ACL***, 2025.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**An empirical study of Ilm-as-a-judge for llm evaluation: Fine-tuned judge model is not a general substitute for gpt-4.**

**H. Huang**, X. Bu, H. Zhou, Y. Qu, X. Bu, J. Liu, M. Yang, B. Xu, and T. Zhao. In ***Findings of ACL***, 2025.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional Supervision.**

S. Li\*, Y. He\*, **H. Huang\***, X. Bu, J. Liu, H. Guo, W. Wang, J. Gu, W. Su, and B. Zheng. In ***Findings of NAACL***, 2025, pp. 8149‚Äì8173.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Self-evaluation of large language model based on glass-box features.**

**H. Huang**, Y. Qu, J. Liu, M. Yang, B. Xu, T. Zhao, and W. Lu. In ***Findings of EMNLP***, 2024, pp. 5813‚Äì5820.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Migitating the bias of large language model evaluation.**

H. Zhou, **H. Huang**, Y. Long, B. Xu, C. Zhu, H. Cao, M. Yang, and T. Zhao. In ***Proc. of CCL***, 2024, pp. 1310‚Äì1319.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Multi-view fusion for instruction mining of large language model.**

**H. Huang**, B. Xu, X. Liang, K. Chen, M. Yang, T. Zhao, and C. Zhu. ***Information Fusion***, vol. 102, p. 102480, 2024.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Improving translation quality estimation with bias mitigation.**

**H. Huang**, S. Wu, K. Chen, H. Di, M. Yang, and T. Zhao. In ***Proc. of ACL***, 2023, pp. 2175-2190.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Iterative nearest neighbour machine translation for unsupervised domain adaptation.**

**H. Huang**, S. Wu, X. Liang, Z. Zhou, M. Yang, and T. Zhao. In ***Findings of ACL***, 2023, pp. 13294-13301.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Multi-view fusion for universal translation quality estimation.**

**H. Huang**, S. Wu, K. Chen, X. Liang, H. Di, M. Yang, and T. Zhao. ***Information Fusion***, vol. 102, p. 102022, 2024.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Towards making the most of Ilm for translation quality estimation.**

**H. Huang**, S. Wu, X. Liang, B. Wang, Y. Shi, P. Wu, M. Yang, and T. Zhao. In ***Proc. of NLPCC***, 2023, pp. 375-386.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**BJTU-toshiba's submission to WMT22 quality estimation shared task.**

**H. Huang**, H. Di, C. Li, H. Wu, K. Ouchi, Y. Chen, J. Liu, and J. Xu. In ***Proc. of WMT***, 2022, pp. 621-626.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Iterative constrained back-translation for unsupervised domain adaptation of machine translation.**

H. Zhang, **H. Huang**, J. Gao, Y. Chen, J. Xu, and J. Liu. In ***Proc. of COLING***, 2022, pp. 5054-5065.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Multi-source inverse-curriculum-based training for low-resource dialogue generation.**

F. Cui, H. Di, **H. Huang**, H. Ren, K. Ouchi, Z. Liu, and J. Xu. ***Applied Intelligence***, vol. 53, pp. 13665-13676, 2022.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Saliency-based multi-view mixed language training for zero-shot cross-lingual classification.**

S. Lai, **H. Huang**, D. Jing, Y. Chen, J. Xu, and J. Liu. In ***Findings of EMNLP***, 2021, pp. 599-610.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Contrastive learning for machine translation quality estimation.**

**H. Huang**, H. Di, J. Liu, Y. Chen, K. Ouchi, and J. Xu. In ***Proc. of NLPCC***, 2021, pp. 92-103.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Ensemble distilling pretrained language models for machine translation quality estimation.**

**H. Huang**, H. Di, J. Xu, K. Ouchi, and Y. Chen. In ***Proc. of NLPCC***, 2020, pp. 231-243.

</div>
</div>

<br><br>
# üéñ Honors and Awards
- *2023*: CHIP Shared Task, Ranked 1st on No-Finetuning Track.
- *2023*: Eval4NLP Shared Task, Ranked 1st on Medium-Sized Track.
- *2022*: WMT Shared Task, Ranked 2nd on Quality Estimation for En-De Tracks.
- *2021*: CCMT Shared Task, Ranked 1st on Quality Estimation and Automatic Post-Editing for Zh-En and Machine Translation for Zh-En, En-Zh and Ti-Zh Tracks.
- *2020*: CCMT Shared Task, Ranked 1st on Quality Estimation for both Zh-En and En-Zh Tracks.