---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

<br><br>
# üëÄ About Me

üå± I‚Äôm **Hui Huang**, a 4th year Ph.D. student at Harbin Institute of Technology in Computer Science.

üìñ My research interest includes:
  - Large Language Models (LLM) 
  - Natural Language Processing (NLP) 

üì´ Feel free to contact me via email: 22b903058@stu.hit.edu.cn.

<br><br>
# üìù Publications

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**H. Huang**, J. Liu, Y. He, S. Li, B. Xu, C. Zhu, M. Yang, T. Zhao. Musc: Improving complex instruction following with multi-granularity self-contrastive training. In ***Proc. of of ACL***, 2025, Vienna, Austra, July. 2025.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**H. Huang**, Y. Qu, X. Bu, H. Zhou, J. Liu, M. Yang, B. Xu, and T. Zhao. An empirical study of Ilm-as-a-judge for llm evaluation: Fine-tuned judge model is not a general substitute for gpt-4. In ***Findings of ACL***, 2025, Vienna, Austra, July. 2025.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

S. Li*, Y. He*, **H. Huang***, X. Bu, J. Liu, H. Guo, W. Wang, J. Gu, W. Su, and B. Zheng. 2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional Supervision. In ***Findings of NAACL***, Albuquerque, New Mexico, April. 2025.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**H. Huang**, Y. Qu, J. Liu, M. Yang, B. Xu, T. Zhao, and W. Lu. Self-evaluation of large language model based on glass-box features. In ***Findings of EMNLP***, Miami, Florida, Nov. 2024.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

H. Zhou, **H. Huang**, Y. Long, B. Xu, C. Zhu, H. Cao, M. Yang, and T. Zhao. Migitating the bias of large language model evaluation. In ***Proc. of CCL***, 2024.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**H. Huang**, B. Xu, X. Liang, K. Chen, M. Yang, T. Zhao, and C. Zhu. Multi-view fusion for instruction mining of large language model. ***Information Fusion***, vol. 102, p. 102480, 2024.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**H. Huang**, S. Wu, K. Chen, H. Di, M. Yang, and T. Zhao. Improving translation quality estimation with bias mitigation. In ***Proc. of ACL (Volume 1: Long Papers)***, 2023, pp. 2175-2190.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**H. Huang**, S. Wu, X. Liang, Z. Zhou, M. Yang, and T. Zhao. Iterative nearest neighbour machine translation for unsupervised domain adaptation. In ***Findings of ACL***, 2023, pp. 13294-13301.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**H. Huang**, S. Wu, K. Chen, X. Liang, H. Di, M. Yang, and T. Zhao. Multi-view fusion for universal translation quality estimation. ***Information Fusion***, vol. 102, p. 102022, 2024.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**H. Huang**, S. Wu, X. Liang, B. Wang, Y. Shi, P. Wu, M. Yang, and T. Zhao. Towards making the most of Ilm for translation quality estimation. In ***Proc. of NLPCC***, 2023, pp. 375-386.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**H. Huang**, H. Di, C. Li, H. Wu, K. Ouchi, Y. Chen, J. Liu, and J. Xu. BJTU-toshiba's submission to WMT22 quality estimation shared task. In ***Proc. of WMT***, 2022, pp. 621-626.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

H. Zhang, **H. Huang**, J. Gao, Y. Chen, J. Xu, and J. Liu. Iterative constrained back-translation for unsupervised domain adaptation of machine translation. In ***Proc. of COLING***, 2022, pp. 5054-5065.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

F. Cui, H. Di, **H. Huang**, H. Ren, K. Ouchi, Z. Liu, and J. Xu. Multi-source inverse-curriculum-based training for low-resource dialogue generation. ***Applied Intelligence***, vol. 53, pp. 13665-13676, 2022.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

S. Lai, **H. Huang**, D. Jing, Y. Chen, J. Xu, and J. Liu. Saliency-based multi-view mixed language training for zero-shot cross-lingual classification. In ***Findings of EMNLP***, 2021, pp. 599-610.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**H. Huang**, H. Di, J. Liu, Y. Chen, K. Ouchi, and J. Xu. Contrastive learning for machine translation quality estimation. In ***Proc. of NLPCC***, 2021, pp. 92-103.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**H. Huang**, H. Di, J. Xu, K. Ouchi, and Y. Chen. Ensemble distilling pretrained language models for machine translation quality estimation. In ***Proc. of NLPCC***, 2020, pp. 231-243.

</div>
</div>

<br><br>
# üíª Experiences
- *2025.07 - Present*, Visiting Student, Institute of Science Tokyo (aka. Tokyo Institute of Technology).
- *2024.07 - 2025.06*, Research Internship, Future Life Laboratory, Alibaba Group.
- *2023.09 - 2024.03*, Research Internship, Department of Wenxin Yiyan, Baidu Inc.. 
- *2023.03 - 2023.08*, Research Internship, AI Lab, ByteDance Inc.. 
- *2022.10 - 2023.03*, Research Internship, Lark AI, ByteDance Inc.. 
- *2019.07 - 2022.08*, Research Internship, Toshiba Research and Development Center. 

<br><br>
# üéñ Honors and Awards
- *2023*: CHIP Shared Task, Ranked 1st on No-Finetuning Track.
- *2023*: Eval4NLP Shared Task, Ranked 1st on Medium-Sized Track.
- *2022*: WMT Shared Task, Ranked 2nd on Quality Estimation for En-De Tracks.
- *2021*: CCMT Shared Task, Ranked 1st on Quality Estimation and Automatic Post-Editing for Zh-En and Machine Translation for Zh-En, En-Zh and Ti-Zh Tracks.
- *2020*: CCMT Shared Task, Ranked 1st on Quality Estimation for both Zh-En and En-Zh Tracks.

<br><br>
# üìñ Educations
- *2022.08 - Present*, Ph.D., Harbin Institute of Technology in Computer Science. Supervised by Professor Muyun Yang.
- *2018.09 - 2022.06*, M.Sc., Beijing Jiaotong University in Computer Science. Supervised by Professor Jin'an Xu.
- *2014.09 - 2018.06*, B.Sc., University of Science and Technology Beijing in Computer Science.