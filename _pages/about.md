---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

<br><br>
# üëÄ About Me

Hi,

üå± I‚Äôm **Hui Huang**, a Ph.D. student at Harbin Institute of Technology in Computer Science. I am supervised by Professor Muyun Yang.

üìñ My research interest includes:
  - Large Language Models (LLM)
  - Natural Language Processing (NLP)

üì´ Feel free to contact me via email: 22b903058@stu.hit.edu.cn.

<br><br>
# üìù Publications

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional Supervision**
S. Li*, Y. He*, **H. Huang***, X. Bu, J. Liu, H. Guo, W. Wang, J. Gu, W. Su, and B. Zheng. *arXiv preprint arXiv:2410.19720*, 2024.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Self-evaluation of large language model based on glass-box features**
**H. Huang**, Y. Qu, J. Liu, M. Yang, B. Xu, T. Zhao, and W. Lu. In *Findings of EMNLP*, Miami, Florida, Nov. 2024.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**An empirical study of Ilm-as-a-judge for llm evaluation: Fine-tuned judge model is not a general substitute for gpt-4**
**H. Huang**, Y. Qu, X. Bu, H. Zhou, J. Liu, M. Yang, B. Xu, and T. Zhao. *arXiv preprint arXiv:2403.02839*, 2024.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Migitating the bias of large language model evaluation**
H. Zhou, **H. Huang**, Y. Long, B. Xu, C. Zhu, H. Cao, M. Yang, and T. Zhao. In *Proc. of CCL*, 2024.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Multi-view fusion for instruction mining of large language model**
**H. Huang**, B. Xu, X. Liang, K. Chen, M. Yang, T. Zhao, and C. Zhu. *Information Fusion*, vol. 102, p. 102480, 2024.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Improving translation quality estimation with bias mitigation**
**H. Huang**, S. Wu, K. Chen, H. Di, M. Yang, and T. Zhao. In *Proc. of ACL (Volume 1: Long Papers)*, 2023, pp. 2175-2190.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Iterative nearest neighbour machine translation for unsupervised domain adaptation**
**H. Huang**, S. Wu, X. Liang, Z. Zhou, M. Yang, and T. Zhao. In *Findings of ACL*, 2023, pp. 13294-13301.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Multi-view fusion for universal translation quality estimation**
**H. Huang**, S. Wu, K. Chen, X. Liang, H. Di, M. Yang, and T. Zhao. *Information Fusion*, vol. 102, p. 102022, 2024.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Towards making the most of Ilm for translation quality estimation**
**H. Huang**, S. Wu, X. Liang, B. Wang, Y. Shi, P. Wu, M. Yang, and T. Zhao. In *Proc. of NLPCC*, 2023, pp. 375-386.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**BJTU-toshiba's submission to WMT22 quality estimation shared task**
**H. Huang**, H. Di, C. Li, H. Wu, K. Ouchi, Y. Chen, J. Liu, and J. Xu. In *Proc. of WMT*, 2022, pp. 621-626.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Iterative constrained back-translation for unsupervised domain adaptation of machine translation**
H. Zhang, **H. Huang**, J. Gao, Y. Chen, J. Xu, and J. Liu. In *Proc. of COLING*, 2022, pp. 5054-5065.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Multi-source inverse-curriculum-based training for low-resource dialogue generation**
F. Cui, H. Di, **H. Huang**, H. Ren, K. Ouchi, Z. Liu, and J. Xu. *Applied Intelligence*, vol. 53, pp. 13665-13676, 2022.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Saliency-based multi-view mixed language training for zero-shot cross-lingual classification**
S. Lai, **H. Huang**, D. Jing, Y. Chen, J. Xu, and J. Liu. In *Findings of EMNLP*, 2021, pp. 599-610.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Contrastive learning for machine translation quality estimation**
**H. Huang**, H. Di, J. Liu, Y. Chen, K. Ouchi, and J. Xu. In *Proc. of NLPCC*, 2021, pp. 92-103.

</div>
</div>

<div class='paper-box'>
<div class='paper-box-text' markdown="1" style="width: 100%;">

**Ensemble distilling pretrained language models for machine translation quality estimation**
**H. Huang**, H. Di, J. Xu, K. Ouchi, and Y. Chen. In *Proc. of NLPCC*, 2020, pp. 231-243.

</div>
</div>

<br><br>
# üíª Experiences
- *2025.07 - Present*, Visiting Student, Institute of Science Tokyo (aka. Tokyo Institute of Technology).
- *2024.07 - 2025.06*, Research Internship, Future Life Laboratory, Alibaba Group.
- *2023.09 - 2024.03*, Research Internship, Department of Wenxin Yiyan, Baidu Inc.
- *2023.03 - 2023.08*, Research Internship, AI Lab, ByteDance Inc.
- *2022.10 - 2023.03*, Research Internship, Lark AI, ByteDance Inc.
- *2019.07 - 2022.08*, Research Internship, Toshiba Research and Development Center.

<br><br>
# üéñ Honors and Awards
- *2024*: EMNLP2024 Worst Paper.
- *2023*: CHIP Shared Task, Ranked 1st on No-Finetuning Track.
- *2023*: Eval4NLP Shared Task, Ranked 1st on Medium-Sized Track.
- *2022*: WMT Shared Task, Ranked 2nd on Quality Estimation for En-De Tracks.
- *2021*: CCMT Shared Task, Ranked 1st on Quality Estimation and Automatic Post-Editing for Zh-En and Machine Translation for Zh-En, En-Zh and Ti-Zh Tracks.
- *2020*: CCMT Shared Task, Ranked 1st on Quality Estimation for both Zh-En and En-Zh Tracks.

<br><br>
# üìñ Educations
- *2022.08 - Present*, Ph.D., Harbin Institute of Technology in Computer Science. Supervised by Professor Muyun Yang.
- *2018.09 - 2022.06*, M.Sc., Beijing Jiaotong University in Computer Science. Supervised by Professor Jin'an Xu.
- *2014.09 - 2018.06*, B.Sc., University of Science and Technology Beijing in Computer Science.